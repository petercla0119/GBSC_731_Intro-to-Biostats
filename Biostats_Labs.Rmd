---
title: "R Notebook"
output: 
  html_notebook: 
    highlight: tango
---
# Lab 1 - Dowloading R and RStudio - No new code associated with lab
# Lab 2 - Working with Data in R
```{r Working with Data in R, echo=FALSE}
# This is a comment
iris <- iris
View(iris)

View(x = iris)

#Extract a single column
iris$Sepal.Length

#Save column as a separate variable
sep_len <- iris$Sepal.Length

#Run a function on a single column in df
mean(x = iris$Sepal.Length)

#Run a function on the 'sep_len' object
mean(x = sep_len) #Output should be identical to output from line 14

install.packages("tidyverse")
library(tidyverse)

#Use filter function in dplyr
# Base R command: iris[iris$Sepal.Length <5, ]
filter(iris, Sepal.Length < 5)

#Save the filtered output as a new variable
filtered_iris <- filter(iris, Sepal.Length < 5)

#Filter the iris dataset
# Base R command: iris[iris$Species == "setosa", ]
filter(iris, Species == "setosa")

#Filter iris dataset for the 'setosa' species and a 'Sepal.Length' less than 5
# Base R command: iris[iris$Species == "setosa", ]
filter(iris, Species == "setosa" & Sepal.Length < 5)


#Use the mutate() function
# Base R command: iris$new_data1 <- iris$Sepal.Length*2
mutate(iris, new_data1 = Sepal.Length *2)

#Add new columns 
mutate(iris, new_data1 = Sepal.Length *2,
       new_data2 = Sepal.Length/Sepal.Width) 


#Use the select() function
# Base R command: iris$Species
select(iris, Species) #selects the Species column and maintains the data structure


#Calculate the mean of Sepal.Length.
# Base R command: mean(iris$Sepal.Length)
summarise(iris, mean = mean(Sepal.Length))

#Calculate the mean, standard deviation and variance of Sepal.Length simultaneously
# Base R command: mean(iris$Sepal.Length); sd(iris$Sepal.Length); var(iris$Sepal.Length)
summarise(iris, mean = mean(Sepal.Length),
          sd = sd(Sepal.Length),
          variance = var(Sepal.Length)) #calculates the mean, standard deviation and variance of Sepal.Length and assigns them to a specified name for easy identification


#Use the arrange() function
# arrange(iris, Sepal.Length). This does the same thing as below
iris %>% arrange(Sepal.Length) # NOTE: ascending is default

#Arrange data in descending order
iris %>% arrange(desc(Sepal.Length)) # does the same thing as above but in descending order of Sepal.Length.

```

# Lab 3 - Plotting Sample Data Using ggplot

# Lab 4 - Plotting Real Data Using ggplot

# Lab 5 - Central Limit Theorem

# Lab 6 - Normality and Sample Properties 1

# Lab 7 - Normality and Sample Properties 2
```{r Normality and Sample Properties 2, echo=FALSE}
# Setup and Import ----
library(car)
library(pastecs)
library(tidyverse)
library(ggplot2)

# setwd("~/Module_4_Assumptions_Correlation/")
# Import data from the Allen Brain Institue 
load("ABI.RData")
source("functions.R")

summary(ABI)
# Histogram
ggplot(data = ABI, mapping = aes(x = ptau)) + 
  geom_histogram(mapping = aes(y = ..density..), bins = 30, fill = 'gray', color = 'black') + 
  stat_function(fun = dnorm, args = list(mean = mean(ABI$ptau, na.rm = TRUE), sd = sd(ABI$ptau, na.rm = TRUE)), color = 'red')

# Q-Q plot
ggplot(data = ABI, mapping = aes(sample = ptau)) + 
  geom_qq() + 
  geom_qq_line()

# Boxplot
ggplot(data = ABI, mapping = aes(x = ptau)) + 
  geom_boxplot() +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())

# Histogram
ggplot(data = ABI, mapping = aes(x = ptau)) + 
  geom_histogram(bins = 30, fill = 'gray', color = 'black') +
  facet_wrap(~ struct, scales = "free")

# Q-Q plot
ggplot(data = ABI, mapping = aes(sample = ptau)) + 
  geom_qq() + 
  geom_qq_line() +
  facet_wrap(~ struct, scales = "free")

# Boxplot
ggplot(data = ABI, mapping = aes(x = ptau)) + 
  geom_boxplot() +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_wrap(~ struct, scales = "free")


# Test Normality -----
stat.desc.clean(dataset = ABI, variable = ptau)
stat.desc.clean(dataset = ABI, variable = ptau, struct)

# Transform to Normal distribution
ABI2 <- ABI %>% 
  mutate(ptau_sqrt = sqrt(ptau),
         ptau_log = log(ptau)) %>% 
  select(ID, struct, ptau, ptau_sqrt, ptau_log) %>% 
  pivot_longer(cols = ptau:ptau_log,
               names_to = "Measurement",
               values_to = "Concentration")

# Histogram
ggplot(data = ABI2, mapping = aes(x = Concentration)) + 
  geom_histogram(bins = 30, fill = 'gray', color = 'black') +
  facet_wrap(~ struct * Measurement, scales = "free")

# Q-Q plot
ggplot(data = ABI2, mapping = aes(sample = Concentration)) + 
  geom_qq() + 
  geom_qq_line() +
  facet_wrap(~ struct * Measurement, scales = "free")

# Boxplot
ggplot(data = ABI2, mapping = aes(x = Concentration)) + 
  geom_boxplot() +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_wrap(~ struct * Measurement, scales = "free")

stat.desc.clean(dataset = ABI2, variable = Concentration, struct, Measurement)

# Homogeneity of Variance
leveneTest(ptau ~ struct, data = ABI)


mip_ABI <- ABI %>% 
  mutate(mip_sqrt = sqrt(mip_1a),
         mip_log = log(mip_1a)) %>% 
  select(ID, struct, mip_1a, mip_sqrt, mip_log) %>% 
  pivot_longer(cols = mip_1a:mip_log,
               names_to = "Measurement",
               values_to = "Concentration")

# Histogram
ggplot(data = mip_ABI, mapping = aes(x = Concentration)) + 
  geom_histogram(bins = 30, fill = 'gray', color = 'black') +
  facet_wrap(~ struct * Measurement, scales = "free")

# Q-Q plot
ggplot(data = mip_ABI, mapping = aes(sample = Concentration)) + 
  geom_qq() + 
  geom_qq_line() +
  facet_wrap(~ struct * Measurement, scales = "free")

# Boxplot
ggplot(data = mip_ABI, mapping = aes(x = Concentration)) + 
  geom_boxplot() +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_wrap(~ struct * Measurement, scales = "free")

stat.desc.clean(dataset = mip_ABI, variable = Concentration, struct, Measurement)

# Homogeneity of Variance
leveneTest(mip_sqrt ~ struct, data = mip_ABI)

```


# Lab 8 - Correlation
```{r Correlation, echo=FALSE}
# Objective: Perform basic correlation analysis with base R cor function and review Pearson's, Spearman's, and Kendall's correlations.

```


# Lab 9 - Linear Regression 1
```{r Lab 9. Loading pkgs and data}
library(tidyverse)
theme_set(theme_bw())
setwd("./Module_5_Linear_Regression/")
load("Module_5_Linear_Regression/framingham.RData")
source("Module_3_CLT-and-Normality/functions.R")
```

```{r Lab 9. Getting to know your data}
head(fhs)
summary(fhs)

# Transforming the data to make it easy to plot with facets
fhs2 <- fhs %>% 
  select(-gender) %>% 
  pivot_longer(cols = age:glucose,
               names_to = "Metric",
               values_to = "Value")

# Distribution of the variables
ggplot(data = fhs2, mapping = aes(x = Value)) + 
  geom_histogram(bins = 30, fill = 'gray', color = 'black') +
  facet_wrap(~ Metric, scales = "free")

# Relationship between the variables
fhs3 <- fhs %>% 
  select(-gender) %>% 
  pivot_longer(cols = c(age, glucose),
               names_to = "Metric",
               values_to = "Value")

ggplot(data = fhs3, mapping = aes(x = Value, y = sysBP)) +
  geom_point() +
  facet_wrap(~ Metric, scales = "free")
```

```{r Lab 9. Assumptions}
# We will review this after the next class
```

```{r Lab 9. Simple linear regression}
# Formula outcome (dependent) ~ predictor (independent), y ~ x

# Model systolic BP as a function of age
sysBPvsAge <- lm(formula = sysBP ~ age, data = fhs)

summary(sysBPvsAge)

# Subset the coefficients table by subsetting the model summary
summary(sysBPvsAge)$coefficients
# sysBPvsAge$coefficients is equivalent to sysBPvsAge[["coefficients"]] and stats::coef(sysBPvsAge)
```


# Lab 13. ANCOVA
```{r Lab 13. ANCOVA}
library(car)
library(ez)
library(rstatix)
library(pastecs)
library(QuantPsyc)
library(tidyverse)
setwd("./Module_7_ANOVA_GLM")

load("/Users/claireps/GBSC_731_Intro-to-Biostats/Module_7_ANOVA_GLM/ANCOVA_Data.RData")
source("/Users/claireps/GBSC_731_Intro-to-Biostats/Module_3_CLT-and-Normality/functions.R")
```

## Getting to Know Your Data
```{r}
summary(mouse.mass) 

ggplot(data = mouse.mass, mapping = aes(x = group, y = bodyfat)) + 
  geom_bar(mapping = aes(fill = group), stat = "summary", fun = "mean", show.legend = FALSE) + 
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.3) + 
  geom_jitter(width = 0.3) + 
  labs(title = "Mean body fat in mice derived from different environments")

ggplot(data = mouse.mass, mapping = aes(x = group, y = leanmass)) + 
  geom_bar(mapping = aes(fill = group), stat = "summary", fun = "mean", show.legend = FALSE) + 
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.3) + 
  geom_jitter(width = 0.3) + 
  labs(title = "Mean lean mass in mice derived from different environments")
```

## Test Assumptions
```{r}
## Homogeneity of Regression Slopes and Linearity ----
ggplot(data = mouse.mass, aes(x = leanmass, y = bodyfat, color = group)) +
  geom_point() + 
  geom_smooth(method = 'lm', se = FALSE)


mouse.mass %>% 
  filter(group == 'wild') %>% 
  lm(bodyfat ~ leanmass, data = .) %>% 
  lm.beta()

mouse.mass %>% 
  filter(group == 'wild-der') %>% 
  lm(bodyfat ~ leanmass, data = .) %>% 
  lm.beta()

mouse.mass %>% 
  filter(group == 'lab') %>% 
  lm(bodyfat ~ leanmass, data = .) %>% 
  lm.beta()
```

## Running ANOVA
### Regular ANOVA
```{r}
mod <- ezANOVA(data = mouse.mass,
               dv = bodyfat,
               between = group,
               wid = idnum,
               type = 3,
               return_aov = TRUE)

mod

# Extracting residuals
residuals <- tibble(group = mouse.mass$group,
                    resid = resid(mod$aov))

# Normality within groups
ggplot(data = residuals, mapping = aes(x = resid)) + 
  geom_histogram(bins = 20, fill = 'gray', color = 'black') +
  facet_wrap(~ group, scales = "free")

ggplot(data = residuals, mapping = aes(sample = resid)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~ group, scales = "free")

ggplot(data = residuals, mapping = aes(x = resid)) +
  geom_boxplot() +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_wrap(~ group, scales = "free")

stat.desc.clean(dataset = residuals, variable = resid, group)
```

### ANCOVA
```{r}
mod <- anova_test(data = mouse.mass,
                  dv = bodyfat,
                  between = group,
                  covariate = leanmass,
                  wid = idnum,
                  type = 3)

mod

# Extracting residuals
residuals <- tibble(group = mouse.mass$group,
                    resid = resid(attributes(mod)$args$model))

# Normality within groups
ggplot(data = residuals, mapping = aes(x = resid)) + 
  geom_histogram(bins = 20, fill = 'gray', color = 'black') +
  facet_wrap(~ group, scales = "free")

ggplot(data = residuals, mapping = aes(sample = resid)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~ group, scales = "free")

ggplot(data = residuals, mapping = aes(x = resid)) +
  geom_boxplot() +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_wrap(~ group, scales = "free")

stat.desc.clean(dataset = residuals, variable = resid, group)

# Homogeneity of variance - wasn't tested by anova_test
leveneTest(resid ~ group, data = residuals)
```
# Lab 15. Factorial ANOVA/Two-way independent ANOVA
```{r}
library(car)
library(ez)
library(pastecs)
library(tidyverse)

theme_set(theme_bw())

setwd("~/GBSC_731_Intro-to-Biostats/Module_8_ANOVA_3")

load("~/GBSC_731_Intro-to-Biostats/Module_8_ANOVA_3/Factorial_ANOVA.RData")

source("~/GBSC_731_Intro-to-Biostats/functions.R")
```

```{r}
ggplot(data = ad2, mapping = aes(x = drug.tx, y = mmse)) + 
  geom_bar(mapping = aes(fill = health), stat = "summary", fun = "mean", show.legend = FALSE) + 
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.3) + 
  geom_jitter(width = 0.3) +
  facet_wrap(~ health)
```

```{r}
# Two-way ANOVA with interaction with the stats or car pkg
mod <- stats::aov(mmse ~ health * drug.tx,
  data = ad2)
summary(mod)

car::Anova(mod)

mod <- ez::ezANOVA(data = ad2,
               dv = mmse,
               between = .(health, drug.tx),
               wid = ID,
               type = 3,
               return_aov = TRUE)

mod
```

```{r}
# Extracting residuals column
residuals <- tibble(group1 = ad2$health,
                    group2 = ad2$drug.tx,
                    resid = resid(mod$aov))

# Normality within groups
ggplot(data = residuals, mapping = aes(x = resid)) + 
  geom_histogram(mapping = aes(y = ..density..), bins = 10, fill = 'gray', color = 'black') +
  facet_wrap(~ group1 + group2, scales = "free")
```

```{r}
ggplot(data = residuals, mapping = aes(sample = resid)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~ group1 + group2, scales = "free")
```

```{r}
ggplot(data = residuals, mapping = aes(x = resid)) +
  geom_boxplot() +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_wrap(~ group1 + group2, scales = "free")
```

```{r}
stat.desc.clean(dataset = residuals, variable = resid, group1, group2)
```

# Lab 19 - Non-Parametric Tests

## Setup

```{r}
library(car)
library(pastecs)
library(tidyverse)

theme_set(theme_bw())

load("/Users/claireps/GBSC_731_Intro-to-Biostats/Module_11/Non-Parametric-Data.RData")

source("Module_3_CLT-and-Normality/functions.R")
```

## Data Exploration

Non-Parametric-Data.Rdata. There are **3 different datasets**: `ad.2vars`, `ad.3vars`, and `noise.g` `ad.2vars` and `ad.3vars` are drug treatment designs. `noise.g` contains the `noise.level` variable which is a repeated measure \#### ad.2vars Appears that placebo had lower scores than the Low Dose group

```{r}
ggplot(data = ad.2vars, mapping = aes(x = drug.tx, y = mmse)) + 
  geom_bar(mapping = aes(fill = drug.tx), stat = "summary", fun = "mean", show.legend = FALSE) + 
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.3) + 
  geom_jitter(width = 0.3) + 
  labs(title = "Effect of Drug Treatment on mmse score") + 
  theme_bw() + 
  theme(axis.title.x = element_blank())
```

### ad.3vars

Appears that High Dose group had an even higher score in the ad.3vars dataset

```{r}
ggplot(data = ad.3vars, mapping = aes(x = drug.tx, y = mmse)) + 
  geom_bar(mapping = aes(fill = drug.tx), stat = "summary", fun = "mean", show.legend = FALSE) + 
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.3) + 
  geom_jitter(width = 0.3) + 
  labs(title = "Effect of Drug Treatment on mmse score") + 
  theme_bw() + 
  theme(axis.title.x = element_blank())
```

### noise.g

Appears that the “none” noise level had a higher score than the “high” noise level in the noise.g dataset

```{r}
ggplot(data = noise.g, mapping = aes(x = noise.level, y = score)) + 
  geom_bar(mapping = aes(fill = noise.level), stat = "summary", fun = "mean", show.legend = FALSE) + 
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.3) + 
  geom_point(alpha = 0.5) +
  geom_line(mapping = aes(group = ID), alpha = 0.5) + 
  labs(title = "Effect of noise level on score for a perceptual task") + 
  theme_bw() + 
  theme(axis.title.x = element_blank())
```

## Wilcoxon Rank Sum (Mann-Whitney) Test

-   Non-parametric equivalent to the **independent t-test**
    -   Tests the null hypothesis that the two samples likely derive from the same population (i.e. whether they have similar distributions).
        -   Conceptually, tests the *difference in medians* between groups.\
            **Mann-Whitney test** only assumes independence. `wilcox.test()` function takes the same inputs as the `t.test()` function If R version 4.3 or below, use: `wilcox.test(y ~ x, data = dataset, paired = FALSE)` If R version 4.4 or above, use:

```{r}
wilcox.test(mmse ~ drug.tx, data = ad.2vars)
```

> **Do the two samples appear to derive from different populations?** Yes, based on the p-value less than 0.05, we can reject the null hypothesis that the two samples are from the same population and therefore state that there is a significant difference in mmse scores as a function of the drug treatment group.

## Wilcoxon Signed Rank Test

-   Non-parametric equivalent to the **paired t-test** - Conceptually, tests if the *median difference* between groups is zero. If R version 4.3 or below, use:\
    `wilcox.test(score ~ noise.level, data = noise.g, paired = TRUE)` If R version 4.4 or above, use:

    ```{r}
    # The x and y arguments here, correspond to the vector of scores for each sound level
    wilcox.test(x = noise.g %>% 
                  filter(noise.level == "none") %>% 
                  pull(score),
                y = noise.g %>% 
                  filter(noise.level == "high") %>% 
                  pull(score),
                paired = TRUE)
    ```

> **Can we reject the null hypothesis that the median difference between noise levels is 0?** Yes! Based on the Wilcoxon Signed-Rank test, we can reject the null hypothesis and state that there is a significant difference in median score between noise levels.

## Kruskal-Wallis Rank Sum Test

**Kruskal-Wallis** only assumes independence

```{r}
kruskal.test(mmse ~ drug.tx, data = ad.3vars)

```

### Post-Hoc Analysis of a Kruskal-Wallis Test

```{r}
pairwise.wilcox.test(x = ad.3vars$mmse, g = ad.3vars$drug.tx, p.adjust.method = "bonferroni")
```

# Lab 21 - Survival analysis

## Setup

```{r}
library(survival)
library(survminer)
library(ggfortify)
library(tidyverse)

load("/Users/claireps/GBSC_731_Intro-to-Biostats/Module_11//Ex_Survival.RData")
```

## Data Exploration

Get understand distribution of data

```{r}
summary(surv1)
summary(surv2)
```

## Survival Analysis

### Kaplan-Meier Analysis

```{r}
surv_table <- Surv(time = surv1$seizuretime, event = surv1$status.num)

km.fit <- surv_fit(formula = surv_table ~ drug, data = surv1)
summary(km.fit, times = seq(from = 0, to = 35, by = 5))
```

#### Plot Kaplan-Meier Survival Curve

```{r}
ggsurvplot(fit = km.fit, data = surv1, # provide the model and the data
           conf.int = TRUE, # plot shaded confidence intervals
           surv.median.line = 'hv',  # plot horizontal and vertical lines at the median for each curve
           pval = TRUE, # plot the overall p-value
           censor.shape = 124, # change the shape for the censor mark
           censor.size = 7) # change the size for the censor mark
```

**Test** whether there is a **difference in survival** - use `survdiff` function

```{r}
survdiff(formula = surv_table ~ drug, data = surv1)
```

Time at which **50%** of the group remains unaffected for **both** drug treatment levels - use `surv_median` function

```{r}
surv_median(fit = km.fit)
```

### Cox Regression Analysis

Look at the effect of expression of two genes (a-beta and tau) on time-to-death in mice using the `surv2` dataset.

#### Assumptions

-   Independence
-   Proportional hazards\

Pass in the `ab40.tau.intxn` variable that describes the *interaction* of `tau` and `ab40`. This has 4 groups that describe mice that express ab40 only, tau only, neither, or both in a single column. surv_fit does not like having interaction terms in the formula, so the interaction was created as a separate variable.

```{r}
surv_table <- Surv(time = surv2$stoptime, event = surv2$status.num)

surv.curv <- surv_fit(formula = surv_table ~ ab40.tau.intxn, data = surv2)

ggsurvplot(fit = surv.curv, data = surv2,
           conf.int = FALSE,
           censor.shape = 124,
           censor.size = 7,
           legend.labs = c('neither', 'ab', 'tau', 'both'))
```

Create log minus log plot - Uses same `ggsurvplot` function call, but adds `fun = 'cloglog'` and `xlim = c(0.5,30)`. - `cloglog` transforms both axes and excludes 0 from the x-axis since 0 cannot be log transformed. - Why code limits the values on the x-axis to be from 0.5 to 30 (our maximum time in our experiment).

```{r}
ggsurvplot(surv.curv, data = surv2,
           conf.int = FALSE,
           censor.shape = 124,
           censor.size = 7,
           legend.labs = c('neither', 'ab', 'tau', 'both'),
           fun = 'cloglog',
           xlim = c(0.5, 30))
```

> There is no substantial crossover in our graph among any of our lines, so none of the levels need to be dropped. If there is a large amount of crossover, the analysis would need to be done on an individual factor basis, i.e. as a function of tau and ab40 independently.

#### Statistical Test of Proportional Hazards

To test the proportional hazards assumption statistically:\

1\. First, create a Cox model using the `coxph` function. - This function behaves exactly like the `surv_fit` function from above, but the right hand side will have ab40, tau, and their interaction (denoted with the \* function).

```{r}
cox.m <- coxph(formula = surv_table ~ ab40*tau, data = surv2)
```

2\. For the statistical test, pass in the `cox.m` object into the `cox.zph` function.

```{r}
cox.zph(cox.m)
```

> This gives our test statistics and p-values for each of the individual terms in the model as well as a test for the overall model. Notice that here, none of the terms are significant, meaning the assumption of proportional hazards is met. If any of the terms are **significant**, they will **need to be removed** from the model. If there is a discrepancy between the graphical analysis and the statistical outcome, try making the model more conservative by removing a term (need to recheck the survival curves without that term to make sure there is not any significant crossing again), or consult a statistician.

##### Running test

View model summary of the cox model

```{r}
summary(cox.m)
```

> Two tables are included in the output along with a few tests at the bottom.
>
> -   The top table presents the main effect of `ab40` and `tau` on survival with their interaction.
>
>     -   For interpretation, focus on the `exp(coef)` column which gives the hazard ratio for that term, and the `Pr(>|z|)` column which gives the p-value for that effect.
>
> -   The second table repeats the hazard ratio, and shows the flipped hazard ratio with a 95% confidence interval (for the non-flipped hazard ratio).
>
>     -   At the bottom, we have results for the omnibus tests, which simply tell us if the overall model is significant or not.

> The interpretation of this model is: if a mouse expresses tau, they have a significantly higher chance of dying than if a mouse does not express tau regardless of the expression of a-beta. The odds of death are around 4.32 times higher for tau expressing mice than non-expressing mice (this is the hazard ratio).


# Lab 22 - Random Forests

## Setup
```{r}
# install.packages("randomForest")
# install.packages("randomForestExplainer")
# install.packages("tree")
# install.packages("MASS")

# Load libraries
library(randomForest)
library(randomForestExplainer)
library(tree)
library(MASS)

# Set your seed
set.seed(123456)

# Load in the Boston Housing data from the MASS package
data(Boston)
```

## Training
```{r}
# Randomly sample 80% of the observation indices from our data set to determine our training set
train_vec <- sort(sample(1:nrow(Boston), floor(nrow(Boston) * 0.8)))
# Select the remaining sample indices for our testing set
test_vec <- setdiff(1:nrow(Boston), train_vec)

# Subset the Boston data set using our training and testing data indices
Boston_train <- Boston[train_vec,]
Boston_test <- Boston[test_vec,]
```

## Basic Regression Tree
```{r}
basic_Boston_tree <- tree::tree(medv ~ ., 
                                data = Boston_train, #Could also use `data = Boston, subset = train_vec`
                                split = "deviance")   #Deviance is the default loss function, gini index is the other option
summary(basic_Boston_tree)
```
### Visualize Basic Regression Tree
```{r}
plot(basic_Boston_tree); text(basic_Boston_tree)
```

### Tree Pruning
#### Determine/optimize the number of nodes
Plot shows after six nodes, we see a fairly minimal step size in the deviance. This means the first six nodes are the most important for redicing model deviance. Let’s prune our tree, and use this as our pruning criteria as the best argument.
```{r}
prune_Boston_tree <- prune.tree(basic_Boston_tree)
plot(prune_Boston_tree)
```

#### Plot the Pruned Tree
```{r}
prune_Boston_tree <- prune.tree(basic_Boston_tree, best = 6)
plot(prune_Boston_tree)
text(prune_Boston_tree, pretty = 0)
```
### Compare Deviance between Pruned and Unpruned Trees
Note: Can also do cross-validation using cv.tree() - not shown in labs
```{r}
summary(basic_Boston_tree)
summary(prune_Boston_tree)
```

### Prediction
```{r}
basic_Boston_pred <- predict(basic_Boston_tree, newdata = Boston_test)
```
# Plot Relationship between Predicted vs. Real Values
If predictions match the true values, then points should follow this line closely.
```{r}
plot(basic_Boston_pred, Boston_test$medv)
abline(0,1, col="red")
```

#### Calculate MSE on testing dataset
```{r}
basic_MSE <- mean((basic_Boston_pred - Boston_test$medv)^2)
sqrt(basic_MSE)
```

## Random Forest
Note: We can perform bagging (wherein we build FULL trees with all covariates) by setting mtry to the number of predictors, but we won’t bother with that here.
```{r}
Boston_forest <- randomForest(medv ~ ., data = Boston_train, 
                              ntree = 500,  #The number of trees to grow in your forest, default is 500
                              #mtry = ...,  #The number of predictors to sample, determined heuristically by default as 1/3*(p) (reg) or sqrt(p) (classification)
                              importance = TRUE,  #We can also determine importance of variables 
                              )
Boston_forest
```

### Compare Random Forest to a Single Tree - Predict on the test set
```{r}
Boston_forest_pred <- predict(Boston_forest, newdata = Boston_test)
par(mfrow = c(1,2))
plot(Boston_forest_pred, Boston_test$medv, main = "Forest"); abline(0,1, col="red")
plot(basic_Boston_pred, Boston_test$medv, main = "Tree"); abline(0,1, col="red")
```


# Lab 23 - Clustering

## Setup
```{r}
set.seed(123456)
```

### Make First Dataset
```{r}
X1 <- matrix(rnorm(50*2), ncol = 2)
X1[1:25 ,1] <- X1[1:25 ,1] + 3
X1[1:25 ,2] <- X1[1:25 ,2] - 4
```

### Make Second Dataset
```{r}
X2 <- matrix(rnorm(100*2), ncol = 2)
X2[1:20 ,1] <- X2[1:20 ,1] + 3
X2[1:20 ,2] <- X2[1:20 ,2] - 4
X2[71:90 ,1] <- X2[81:90 ,1] - 1
X2[71:90 ,2] <- X2[81:90 ,2] + 1
X2[91:100 ,1] <- X2[91:100 ,1] - 6
X2[91:100 ,2] <- X2[91:100 ,2] + 5
```

## K-means Clustering
```{r}
X1_km_2 <- kmeans(X1, 
                 centers = 2,  #K, the number of centers 
                 nstart = 20   #The number of random sets, defaults to 1 but should always be made bigger
                 )
```
### Check Cluster Assignments
```{r}
X1_km_2$cluster
```

### Plot Clusters
```{r}
plot(X1, col = (X1_km_2$cluster + 1), 
     main="Two Cluster K-Means K=2", 
     xlab = "", ylab = "", pch = 20, cex = 2)
```

### Compare Number of Centers
```{r}
X2_km_2 <- kmeans(X2, center = 2, nstart = 20)
X2_km_3 <- kmeans(X2, center = 3, nstart = 20)
X2_km_4 <- kmeans(X2, center = 4, nstart = 20)


#Comparing all three side-by-side
par(mfrow = c(1,3))
plot(X2, col = (X2_km_2$cluster + 1), 
     main="Four Cluster K-Means K=2", 
     xlab = "", ylab = "", pch = 20, cex = 2)
plot(X2, col = (X2_km_3$cluster + 1), 
     main="Four Cluster K-Means K=3", 
     xlab = "", ylab = "", pch = 20, cex = 2)
plot(X2, col = (X2_km_4$cluster + 1), 
     main="Four Cluster K-Means K=4", 
     xlab = "", ylab = "", pch = 20, cex = 2)
```
#### Visualize the output of one of the clusterings
```{r}
par(mfrow = c(1,1)) # Reset plotting layout
table(X2_km_4$cluster)
```

Small within-cluster sum of squares indicates tighter clusters


